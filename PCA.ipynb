{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the code depenedencies\n",
    "!pip install -r https://raw.githubusercontent.com/kunalghosh/ML-CSC-tutorial/master/requirements.txt\n",
    "# Download the data\n",
    "!curl https://zenodo.org/record/5535556/files/data.tar.gz -o data.tar.gz\n",
    "# Extract the data\n",
    "!tar -xvzf data.tar.gz\n",
    "# Setup the code \n",
    "!git clone https://github.com/kunalghosh/ML-CSC-tutorial\n",
    "!mv /content/ML-CSC-tutorial/* ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis\n",
    "## Introduction\n",
    "\n",
    "Principal Component Analysis (PCA) is a dimensionality-reduction method that is typically used to transform a high-dimensional data set into a smaller-dimensional subspace prior to applying a machine learning algorithm on the data.\n",
    "\n",
    "\n",
    "Let $\\mathbf{x^i} = \\left[ x_1^i, x_2^i, \\cdots, x_M^i \\right]$ be a M-dimensional vector describing the $i$-th sample. The entire dataset of $N$ samples can be expressed as a $N \\times M$ matrix $X$, whose rows are the samples:\n",
    "\n",
    "\\begin{align}\n",
    "X &= \\left(\n",
    "\\begin{array}{cccc}\n",
    "x_1^1 & x_2^1  & \\ldots & x_M^1  \\\\\n",
    "x_1^2 & x_2^2  & \\ldots & x_M^2  \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "x_1^N & x_2^N  & \\ldots & x_M^N\n",
    "\\end{array} \\right) \n",
    "\\end{align}\n",
    "\n",
    "PCA aims to transform the original data in order to maximise its variance. In practice, the new representation of the data $Y$ is linearly related to the original one:\n",
    "\n",
    "\\begin{equation} \n",
    "Y = X \\cdot V\n",
    "\\end{equation}\n",
    "\n",
    "The columns of $V$, $\\{ \\mathbf{v}_1, \\cdots , \\mathbf{v}_M \\}$ are the principal components (PCs) of $X$, and correspond to the eigenvectors of the covariance matrix of $X$, which is defined as:\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{cov} (X)= \\frac{1}{N} \\sum_n^{N} (x_n - \\mu_x) \\, (x_n - \\mu_x)\n",
    "\\end{equation}\n",
    "\n",
    "The covariance matrix, $\\text{cov} (X)$, describes all relationships between pairs of measurements in our dataset $X$.\n",
    "\n",
    "Because the covariance matrix is symmetric, the eigenvectors form an orthogonal set. The PCs (eigenvectors) correspond to the direction (in the original n-dimensional space) with the greatest variance in the data. Each eigenvector has a corresponding eigenvalue, indicating how much variance there is in the data along the corresponding eigenvector (or PC).\n",
    "\n",
    "The dimensionality reduction can be performed by removing the PCs with the lowest eigenvalues, thus throwing away redundant features with low information content.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial 1: creating PCA function\n",
    "\n",
    "In order to get better insight into PCA, we will create a Python script for PCA from scratch and then apply it on a simple toy data.\n",
    "\n",
    "<b>NOTE</b>: it is important to run all the steps (code cells) below to ensure correct execution.\n",
    "\n",
    "First, the following codes is necessary to import the useful part of Python tools and libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import matplotlib.pyplot as plt # This imports matplotlib library for plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we define the PCA function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PCA(X):\n",
    "    \n",
    "    # computes the covariance of X\n",
    "    # numpy covariance function assumes different ordering, so we transpose X\n",
    "    XCov = np.cov(X.T)\n",
    "    \n",
    "    # solves the eigenproblem and stores eigvals in D and eigvecs in V\n",
    "    D, V = np.linalg.eig(XCov)\n",
    "    \n",
    "    # perform the linear transformation - matrix-matrix multiplication\n",
    "    Yn = np.dot(X,V)\n",
    "\n",
    "    # return the eigenvector matrix V, the transformed data Yn, and the eigenvalues D\n",
    "    return V,Yn,D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial 2: understanding PCA on a simple toy data\n",
    "\n",
    "Now, we will apply PCA function to a simple toy data set.\n",
    "\n",
    "It is crucial to normalize the original data before applying PCA. In the data, some variables (columns of X) may have overall large values while others may be smaller. Without normalization, the variables with large values will be dominating the first PCs.\n",
    "\n",
    "Z-score, one of popular normalization methods, converts all variables to a common scale with an average of zero and standard deviation of one.\n",
    "Z-score is defined mathematically as $\\text{zscore}(x) = (x-\\mu_x)/\\sigma_x$ where $x$ is a column of the dataset $X$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define normalization function here, that is zscore function\n",
    "# It is also known as standard score\n",
    "def zscore(X): # z-score uses to normalise the data.\n",
    "    \n",
    "    # get the shape of the data matrix\n",
    "    [nX,mX] = X.shape\n",
    "    \n",
    "    # compute the mean of every column X\n",
    "    XMean = np.mean(X, axis=0)\n",
    "    \n",
    "    # compute standard deviation of each column\n",
    "    XStd = np.std(X,axis=0,ddof=1)\n",
    "    \n",
    "    # subtract the mean from each column\n",
    "    zX = X - np.kron(np.ones((nX,1)),XMean) # Z = [X - mX]\n",
    "    \n",
    "    # divide by the stdv\n",
    "    Zscore = np.divide(zX,XStd)\n",
    "    \n",
    "    return Zscore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create some toy data and normalize it using <i>zscore</i>.\n",
    "For simplicity, we create a set of points in the plane, roughly arranged as an ellipse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make an array of 13 angle values, from 0 to 2PI, at regular intervals\n",
    "t = np.linspace(0, 2*np.pi, 13) \n",
    "\n",
    "# set some parameters of the ellipse\n",
    "A = 1\n",
    "B = 1.3\n",
    "x0 = 1.5\n",
    "y0 = 0.5\n",
    "angle = -45\n",
    "\n",
    "# compute x and y coordinates from the angles and parameters\n",
    "x = A*np.cos(t+angle)+x0 + 0.05*np.random.uniform(-1.0,1.0,t.size)\n",
    "y = B*np.sin(t)+y0 + 0.05*np.random.uniform(-1.0,1.0,t.size)\n",
    "# put the x and y arrays into a matrix format\n",
    "Xdata=np.column_stack([x,y])\n",
    "\n",
    "# Next, we normalize the data\n",
    "X = zscore(Xdata) \n",
    "\n",
    "# Plot the original data first (normalized)\n",
    "plt.figure(1)\n",
    "plt.plot(Xdata[:,0],Xdata[:,1],'ro', label='original') # re-plot the data\n",
    "plt.plot(X[:,0],X[:,1],'bo',label='zscored') # re-plot the data\n",
    "plt.legend()\n",
    "plt.xlabel('X1')\n",
    "plt.ylabel('X2')\n",
    "\n",
    "plt.title('Original data prior to PCA transformation')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.ylim(bottom=-2.0, top=3.0)\n",
    "plt.xlim(left=-2, right=3)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we apply <i>PCA</i> function to the data and print eigenvalues associated with two PCs. The results indicate that the highest variance is held in the first PC with variance of ~1.83 while the second PC has variance of ~0.16."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying PCA\n",
    "V,Ypca,D = PCA(X)            # perform PCA, where V, Ypca and D are\n",
    "                             # eigenvectors, new transformation and eigenvalues,\n",
    "                             # respectively\n",
    "print(\"Eigenvalues D: %s\" % D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA transforms the original data space to PC space where the highest information is held by the first PC. PCA will be extremely beneficial when we deal with high dimensional data sets, so we can discard unneccesary PCs which contain very low variance. \n",
    "This type of problem will be tested on real chemical data in the final tutorial.\n",
    "The following code is to plot the original data together with the new transformation data as well\n",
    "as their eigenvector (PCs) as the direction. The produced Figure demonstrates how the original data (blue\n",
    "dots) rotate to form new transformation data (red dots)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We re-plot the original Figure\n",
    "plt.figure(1)\n",
    "plt.plot(X[:,0],X[:,1],'bo', label='zscored data') # re-plot the data\n",
    "plt.xlabel('X1')\n",
    "plt.ylabel('X2')\n",
    "\n",
    "plt.plot(Ypca[:,0],Ypca[:,1],'ro', label='PCA transformed')\n",
    "plt.title('After PCA transformation')\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.ylim(bottom=-2.0, top=2.0)\n",
    "plt.xlim(left=-2, right=2)\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: dimensionality reduction on wine chemical data\n",
    "\n",
    "In this tutorial we will use real experimental data, representing the chemical composition of about 178 wine samples, from three different cultivars in the same region in Italy.\n",
    "For each wine, the analysis determined the quantities of 13 constituents, listed below:\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <th>Feature No.</th>\n",
    "    <th>Wine Chemical Composition</th> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>1</td>\n",
    "    <td>Alcohol</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>2</td>\n",
    "    <td>Malic acid</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>3</td>\n",
    "    <td>Ash</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>4</td>\n",
    "    <td>Alcalinity of ash</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>5</td>\n",
    "    <td>Magnesium</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>6</td>\n",
    "    <td>Total phenols</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>7</td>\n",
    "    <td>Flavanoids</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>8</td>\n",
    "    <td>Nonavanoid phenols</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>9</td>\n",
    "    <td>Proanthocyanins</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>10</td>\n",
    "    <td>Color intensity</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>11</td>\n",
    "    <td>Hue</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>12</td>\n",
    "    <td>OD280/OD315 of diluted wines</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>13</td>\n",
    "    <td>Proline</td>\n",
    "  </tr>\n",
    "  <caption>Wine chemical composition*\n",
    "  \n",
    "  *Lichman, M. (2013). UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. Irvine, CA: University of California, School of Information and Computer Science</caption>\n",
    "</table><br>\n",
    "\n",
    "The data file <i>./data/wineInputs.txt</i> contains 178 lines, each with the 13 entries representing the wine chemical composition (descriptor). The file  <i>./data/wineOutputs.txt</i> contains the type of each wine: either 1, 2 or 3.\n",
    "\n",
    "In this tutorial, the objective is to reduce the dimensionality of the descriptors by identifying and eliminating the redundant ones with PCA. \n",
    "\n",
    "Fill in the missing code based on the previous examples.\n",
    "This tutorial will be used later for the exercise on the following topics (Kmeans, KNN and Neural Network)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we load input/output data from the files\n",
    "dataIn = np.genfromtxt('./data/wineInputs.txt', delimiter=',')\n",
    "dataOut = np.genfromtxt('./data/wineOutputs.txt', delimiter=',')\n",
    "\n",
    "## # Apply normalization\n",
    "# ...\n",
    "\n",
    "## # Apply PCA\n",
    "# ...\n",
    "\n",
    "\n",
    "# Here, we try to obtain the normalized cumulative sum (cumsum) of eigenvalues\n",
    "# assuming D are the eigenvalues from PCA\n",
    "idc = np.divide(np.cumsum(D),np.sum(D)) \n",
    "\n",
    "# We plot normalized cumulative sum to understand the contributions of the obtained PCs\n",
    "plt.title('Normalized cumulative sum')\n",
    "plt.xlabel('# principal components')\n",
    "plt.ylabel('cumulative sum')\n",
    "\n",
    "plt.plot(range(1,len(D)+1),idc,'bo') # re-plot the data\n",
    "\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
